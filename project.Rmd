---
title: "Practical Machine Learning - Project Writeup"
author: "Yilun Zhang"
date: "Sunday, November 09, 2014"
output:
  html_document:
    keep_md: yes
---

```{r, cache=TRUE}
setwd("F:/Documents/University/3A 2014 WT4 Fall/coursera/Data Science 8 - Practical Machine Learning/Project-Writeup")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

```{r}
library(caret)
library(nnet)
library(pROC)
```

## Exploratory Analysis and Preprocessing

Check missing values in the data. Remove columns with more than 10,000 missing values.

```{r, cache=TRUE}
training2 <- training[,colSums(is.na(training))<10000]
```

Check blank values in the data. Remove columns with more than 10,000 blanks.

```{r}
blankValues <- apply(training2,2,function(x) sum(x == ""))
training3 <- training2[names(blankValues[blankValues < 10000])]
```

By looking at the first few columns, I noticed that some of them are information columns, so I decided to remove them from the following analyses (first 5 columns).

```{r}
training4 <- training3[,-c(1:5)]
```

Up to this point, we have done all the data cleaning work and we will start our analysis. We will then separate the training set into two sets: model fitting set and prediction accuracy checking set. The purpose for separating the training set is that, the given test set doesn't have the classe column, so it is beneficial for us to use part of the training set as a test set to calculate the accuracy of our model prediction.

```{r}
inTrain <- createDataPartition(y=training4$classe,p=0.9,list=F)
train <- training4[inTrain,]
check <- training4[-inTrain,]
```

This marks the end of data cleaning, and we will move on to the model construction.

## Model Construction and Accuracy Analysis

#### 1st Model

The first model I come up with is the decision trees model, the following is the model fitting and prediction accuracy analysis.

```{r, cache=TRUE}
modFit1 <- train(classe~.,data=train,method="rpart")
table(predict(modFit1,check),check$classe)
p1 <- predict(modFit1,check)
round(table(p1,check$classe)/rowSums(table(p1,check$classe))*100,digit=2)
sum(predict(modFit1,check)==check$classe)/length(check$classe)
pred1 <- predict(modFit1,testing)
```

#### 2nd Model

The second model I come up with is the multinomial classification using the generalized linear model. The following is the model fitting and prediction accuracy analysis.

```{r, cache=TRUE}
modFit2 <- multinom(classe~.,data=train)
table(predict(modFit2,check),check$classe)
p2 <- predict(modFit2,check)
round(table(p2,check$classe)/rowSums(table(p2,check$classe))*100,digit=2)
sum(predict(modFit2,check)==check$classe)/length(check$classe)
pred2 <- predict(modFit2,testing)
```

#### 3rd Model

The third model I come up with is the k-th nearest neighbour model. The following is the model construction and accuracy calculation.

```{r, cache=TRUE}
modFit3 <- train(classe~.,data=train,method="knn")
table(predict(modFit3,check),check$classe)
p3 <- predict(modFit3,check)
round(table(p3,check$classe)/rowSums(table(p3,check$classe))*100,digit=2)
sum(predict(modFit3,check)==check$classe)/length(check$classe)
pred3 <- predict(modFit3,testing)
```

## Discussion

In the last section, we have created 3 models:

1. Classification trees 
2. Generalized linear model
3. K-th nearest neighbour

By judging the performance with the prediction accuracy, k-th nearest neighbour beats the other two with an accuracy of 94.6% versus 52.3% for classfication tree and 63.7% for glm.

Therefore, I will be using the knn model for predicting the classe in testing set.

Now, let's take a look at the variable importance in the knn model (which is generated by using the ROC curve in pROC package).

```{r, cache=TRUE}
imp <- varImp(modFit3)$importance
imp$sum <- imp$A + imp$B + imp$C + imp$D + imp$E
imp <- imp[order(-imp$sum),]
head(imp,n=10)
```

The above table shows the top 10 important variables in the knn predictive model.

```{r}
qplot(pitch_forearm,roll_dumbbell,colour=classe,data=train,
      main="Classification for training data\n")
qplot(pitch_forearm,roll_dumbbell,colour=pred3,data=testing,size=I(5),
      main="Classification for testing data\n")
```

## Prediction

Below is the final prediction for the 20 cases in the testing set.
```{r}
pred3
```

The following code will produce 20 files, each contains 1 single character representing the final prediction for the associated problem-id.

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

setwd("predictions")
pml_write_files(pred3)
```

## Conclusion

With the application of machine learning concepts, among the 3 models applied to our data, the k-th nearest neighbour model has the best prediction result with an accuracy of 94.6%.

By submitting the predicted output files to Coursera, we have achieved an accuracy of 90% (18 correct out of 20).
